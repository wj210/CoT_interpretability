# CoT_interpretability

This project is based on 3 steps

**1) Generating base CoT reasoning chains**
- Set prompt_type to [`cot`, `cot_sc`, `cot_qd`, `cot_refine`, `cot_sec`], where `cot_sec` is the proposed entailment alignment approach
- Set dataset to [`obqa`, `qasc` `strategyqa`]
- 
